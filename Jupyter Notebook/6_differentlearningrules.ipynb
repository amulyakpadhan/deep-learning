{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights: [[0.]\n",
      " [0.]]\n",
      "Initial Bias: [[0.]]\n",
      "w:  [[0.]\n",
      " [0.]]\n",
      "b:  [[0.]]\n",
      "o:  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Output after training:\n",
      "[[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # XOR input\n",
    "Y = np.array([[0], [1], [1], [0]])\n",
    "weights = np.array([[0.0], [0.0]])\n",
    "bias = np.array([[0.0]])\n",
    "print(\"Initial Weights:\", weights)\n",
    "print(\"Initial Bias:\", bias)\n",
    "learning_rate = 0.1  # Learning rate\n",
    "\n",
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def train(X, Y, iterations):\n",
    "    global weights, bias\n",
    "    for _ in range(iterations):\n",
    "        # Step 4: activation function (output)\n",
    "        output = sigmoid(np.dot(X, weights) + bias)\n",
    "\n",
    "        # Calculate error\n",
    "        error = Y - output\n",
    "\n",
    "        # Step 5: Adjust weights (Gradient Descent)\n",
    "        adjustments = error * sigmoid_derivative(output)\n",
    "        weights += np.dot(X.T, adjustments) * learning_rate\n",
    "        bias += np.sum(adjustments) * learning_rate\n",
    "        return weights, bias, output\n",
    "        # Step 7: Repeat until no change in weights\n",
    "        if np.all(np.abs(error) == 0):  # stopping condition\n",
    "            print(\"Training stopped.\")\n",
    "            break\n",
    "\n",
    "# Train the network\n",
    "w,b,o=train(X, Y, 10000)\n",
    "print(\"w: \",w)\n",
    "print(\"b: \",b)\n",
    "print(\"o: \",o)\n",
    "\n",
    "# Test the output\n",
    "output = sigmoid(np.dot(X, w) + b)\n",
    "print(\"Output after training:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Initialize the input vectors associated with the target values\n",
    "# Example: AND logic gate problem\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "Y = np.array([0, 0, 0, 1])  # AND gate target values\n",
    "\n",
    "# Step 2: Initialize the weights and bias (randomly)\n",
    "weights = np.random.rand(2)  # Two input neurons, thus two weights\n",
    "bias = np.random.rand(1)[0]  # Single bias term\n",
    "\n",
    "# Step 3: Set learning rule parameters\n",
    "learning_rate = 0.1  # Learning rate\n",
    "threshold = 0.5  # Threshold for binary classification (0 or 1)\n",
    "\n",
    "# Activation function (Step 4)\n",
    "def activation_function(net_input):\n",
    "    return 1 if net_input >= threshold else 0\n",
    "\n",
    "# Step 5, 6 & 7: Calculate output, adjust weights, and iterate until no weight change\n",
    "epoch = 0\n",
    "while True:\n",
    "    epoch += 1\n",
    "    weight_changed = False  # Flag to check if weights have changed\n",
    "    total_error = 0\n",
    "    # print(f\"Epoch {epoch}\")  # This can be optional if you don't want epoch number printed during training\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        # Calculate net input\n",
    "        net_input = np.dot(X[i], weights) + bias\n",
    "\n",
    "        # Calculate output using activation function\n",
    "        output = activation_function(net_input)\n",
    "\n",
    "        # Calculate error\n",
    "        error = Y[i] - output\n",
    "        total_error += error ** 2\n",
    "\n",
    "        # Store the old weights and bias for comparison\n",
    "        old_weights = weights.copy()\n",
    "        old_bias = bias\n",
    "\n",
    "        # Step 6: Adjust weights and bias\n",
    "        weights += learning_rate * error * X[i]\n",
    "        bias += learning_rate * error\n",
    "\n",
    "        # Check if weights or bias changed\n",
    "        if not np.array_equal(weights, old_weights) or bias != old_bias:\n",
    "            weight_changed = True  # Set flag to indicate a change\n",
    "\n",
    "    # Step 7: Stop iterations if there is no change in weights and bias\n",
    "    if not weight_changed:\n",
    "        break\n",
    "\n",
    "# Print the results after training\n",
    "print(\"Training complete.\")\n",
    "print(f\"Final Weights: {weights}\")\n",
    "print(f\"Final Bias: {bias}\")\n",
    "\n",
    "# Optionally, print input and output for final iteration\n",
    "for i in range(len(X)):\n",
    "    # Calculate net input\n",
    "    net_input = np.dot(X[i], weights) + bias\n",
    "    # Calculate output using activation function\n",
    "    output = activation_function(net_input)\n",
    "    print(f\"Input: {X[i]} - Target: {Y[i]} - Predicted: {output}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
