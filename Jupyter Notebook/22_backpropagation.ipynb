{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Error: 0.49721892454240146\n",
      "Epoch 1000, Error: 0.4880611702611545\n",
      "Epoch 2000, Error: 0.42140647208759563\n",
      "Epoch 3000, Error: 0.34147614865219594\n",
      "Epoch 4000, Error: 0.21182296553191785\n",
      "Epoch 5000, Error: 0.12925272319332737\n",
      "Epoch 6000, Error: 0.09344799285513183\n",
      "Epoch 7000, Error: 0.07464046653242815\n",
      "Epoch 8000, Error: 0.06306756377339848\n",
      "Epoch 9000, Error: 0.05518490381142588\n",
      "\n",
      "Final Outputs after Training:\n",
      "[[0.05035392]\n",
      " [0.94687409]\n",
      " [0.95698317]\n",
      " [0.05126862]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'Input1': [0, 0, 1, 1],\n",
    "        'Input2': [0, 1, 0, 1],\n",
    "        'Output': [0, 1, 1, 0]}  # XOR problem\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Inputs and outputs\n",
    "X = df[['Input1', 'Input2']].values  # Input features\n",
    "y = df['Output'].values.reshape(-1, 1)  # Target output\n",
    "\n",
    "# Initialize weights and bias\n",
    "input_layer_size = X.shape[1]  # 2 (2 inputs)\n",
    "hidden_layer_size = 4         # 4 neurons in the hidden layer\n",
    "output_layer_size = 1         # 1 output neuron\n",
    "\n",
    "# Randomly initialize weights and biases\n",
    "np.random.seed(42)\n",
    "weights_input_hidden = np.random.rand(input_layer_size, hidden_layer_size)\n",
    "weights_hidden_output = np.random.rand(hidden_layer_size, output_layer_size)\n",
    "bias_hidden = np.random.rand(1, hidden_layer_size)\n",
    "bias_output = np.random.rand(1, output_layer_size)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 10000\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Training the network\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    hidden_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "    \n",
    "    final_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "    final_output = sigmoid(final_input)\n",
    "    \n",
    "    # Calculate the error (difference between expected and actual output)\n",
    "    error = y - final_output\n",
    "    \n",
    "    # Backpropagation\n",
    "    # Calculate the gradient of the output layer\n",
    "    output_delta = error * sigmoid_derivative(final_output)\n",
    "    \n",
    "    # Calculate the gradient of the hidden layer\n",
    "    hidden_delta = output_delta.dot(weights_hidden_output.T) * sigmoid_derivative(hidden_output)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    weights_hidden_output += hidden_output.T.dot(output_delta) * learning_rate\n",
    "    weights_input_hidden += X.T.dot(hidden_delta) * learning_rate\n",
    "    bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
    "    bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
    "    \n",
    "    # Print the error every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Error: {np.mean(np.abs(error))}\")\n",
    "\n",
    "# After training, test the network on the same inputs\n",
    "print(\"\\nFinal Outputs after Training:\")\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.78%\n",
      "\n",
      "Final weights after training:\n",
      "Layer 1 weights shape: (4, 10)\n",
      "[[-8.79229678e-08  4.52073506e-01  7.94937316e-01  1.90185856e-01\n",
      "  -1.05695789e-02 -4.02436653e-01 -4.00716072e-02  3.11406317e-01\n",
      "   1.85761909e-01  1.63055756e-04]\n",
      " [-5.73336312e-02  6.98356831e-01  9.47852982e-01 -6.35176974e-01\n",
      "  -6.41519979e-03 -6.00503861e-01 -8.16795392e-05 -1.28997515e-01\n",
      "  -5.67842351e-01 -1.69130088e-04]\n",
      " [ 7.56662961e-09 -8.22388012e-01 -5.00006070e-01  3.25807217e-01\n",
      "  -2.09587930e-20  8.55167184e-01 -4.30129880e-03  7.63669219e-02\n",
      "   7.53309960e-01 -4.50819297e-02]\n",
      " [ 2.97926176e-09 -8.27285320e-01 -1.19618796e+00  1.22364522e+00\n",
      "   5.06164328e-02  9.14032336e-01 -7.98771902e-05 -2.07607074e-01\n",
      "   9.73938739e-01  1.47877888e-17]]\n",
      "Layer 2 weights shape: (10, 3)\n",
      "[[ 6.13712464e-02  2.96292875e-03  4.62176305e-02]\n",
      " [ 1.01815228e+00  6.96274001e-02  3.27627213e-01]\n",
      " [ 3.11095849e-02  3.01073148e-02 -1.17933833e+00]\n",
      " [-7.47538604e-01  1.00143358e-01  2.68033604e-01]\n",
      " [ 1.00453781e-02 -1.82641794e-06 -4.09452225e-04]\n",
      " [-7.88845210e-01 -1.00871574e+00  9.67727967e-01]\n",
      " [-3.99021466e-02  7.10545277e-02  2.73352982e-03]\n",
      " [-2.09150666e-01 -4.65887809e-01  2.09827729e-01]\n",
      " [-2.58455725e-01  4.62233562e-01  8.68027118e-01]\n",
      " [-4.01209974e-02 -1.50230879e-06 -2.42373569e-02]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AhadA\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the MLPClassifier\n",
    "# hidden_layer_sizes=(10,) indicates one hidden layer with 10 neurons\n",
    "# max_iter=1000 sets the maximum number of iterations for training\n",
    "# solver='adam' uses the Adam optimization algorithm, which uses backpropagation\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, solver='adam', random_state=42)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print the final weights of the model\n",
    "print(\"\\nFinal weights after training:\")\n",
    "for i, layer_weights in enumerate(mlp.coefs_):\n",
    "    print(f\"Layer {i+1} weights shape: {layer_weights.shape}\")\n",
    "    print(layer_weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
